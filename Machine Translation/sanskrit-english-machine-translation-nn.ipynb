{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6838685fbf7c9f8a419e17523ed6f4c2a8edaab9"
   },
   "outputs": [],
   "source": [
    "mark_start  = 'ssss '\n",
    "mark_end = ' eeee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "d2d5128720ee7665c8dd1709b8d1a65dda4de81f"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ddb843cea72c3c00b461ba63254c10d05bb7202b"
   },
   "outputs": [],
   "source": [
    "def preProcess(data):\n",
    "    data = str(data).lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7b8f8339c9fedac85f6b44f1c99404ca4c40f556"
   },
   "outputs": [],
   "source": [
    "data['english_word'] = mark_start + data['english_word'].astype(str) + mark_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "9aa0f7c54b8bc78b83bfb6f9d6d367f372425a7a"
   },
   "outputs": [],
   "source": [
    "sanskrit_data = data['sanskrit_word'].apply(preProcess)\n",
    "english_data = data['english_word'].apply(preProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "4481f411961fb2931806852981ffdc370f3ab907"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss  the syllable om eeee'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b16ab1a6a906ce69512564eceaf0be5c6ef48599"
   },
   "outputs": [],
   "source": [
    "# naxunyn number of vocabulary.\n",
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "669752566151bdf0f17de795a7cbba06e8a0ef1c"
   },
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    \n",
    "    def __init__(self, texts, padding, reverse=False,num_words=None):\n",
    "        \n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "        \n",
    "        self.fit_on_texts(texts)\n",
    "        \n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                     self.word_index.keys()))\n",
    "        \n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "        \n",
    "        if reverse:\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            truncating = \"pre\"\n",
    "        else:\n",
    "            truncating = \"post\"\n",
    "        \n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "        \n",
    "        self.max_tokens = np.mean(self.num_tokens) + 2 * np.std(self.num_tokens)\n",
    "        \n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "        \n",
    "        self.tokens_padded = pad_sequences(self.tokens, maxlen=self.max_tokens, padding= padding, truncating = truncating)\n",
    "        \n",
    "    def token_to_word(self,token):\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "    \n",
    "    def tokens_to_string(self, tokens):\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        text = \" \".join(words)\n",
    "        return text\n",
    "    \n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.max_tokens,\n",
    "                                   padding='pre',\n",
    "                                   truncating=truncating)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e1044317d5aaa944b821d25b385cd4e6d838c7c7"
   },
   "outputs": [],
   "source": [
    "sanskrit_tokenizer = TokenizerWrap(texts=sanskrit_data,\n",
    "                              padding='pre',\n",
    "                              reverse=True,\n",
    "                              num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7ebfa6799e75ac54414e35f6d681476f173bd9b8"
   },
   "outputs": [],
   "source": [
    "english_tokenizer = TokenizerWrap(texts=english_data,\n",
    "                              padding='post',\n",
    "                              reverse=False,\n",
    "                              num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "0b6b84922d041f3ddbc8cf06a30b3ccc67edc1d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9323, 2)\n",
      "(9323, 10)\n"
     ]
    }
   ],
   "source": [
    "sanskrit_tokens = sanskrit_tokenizer.tokens_padded\n",
    "english_tokens = english_tokenizer.tokens_padded\n",
    "print(sanskrit_tokens.shape)\n",
    "print(english_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "57a40315ccdadf2a2f2874f8f757053f0117ad0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1 2559  122    2    0    0    0    0    0    0]\n",
      " [   1    3 1167 1636    2    0    0    0    0    0]\n",
      " [   1   36    2    0    0    0    0    0    0    0]\n",
      " [   1    8 2560 2561  599   65    5  136 1637    2]\n",
      " [   1  455    3  456 2562   17   53    3  357    2]]\n"
     ]
    }
   ],
   "source": [
    "print(english_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "f7c26969968adcc182806eec5b5218f3355a8845"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = english_tokenizer.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "e21ada0ed0a8fc3dd8c9a724c24f79ff9c16aaf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end = english_tokenizer.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "67a201168b07db9bacd30c639d2cee9a34044a1e"
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "encoder_input_data = sanskrit_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "82928bcb2b198ae62b3a558ca9fc444f7779a701"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9323, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data = english_tokens[:,:-1]\n",
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "8969754ef8684671dbd2842f0faa383d72a84507"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9323, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data = english_tokens[:,1:]\n",
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "b06e7d1ae3c2ad7ca04a93a02c2f1bcba86affbd"
   },
   "outputs": [],
   "source": [
    "#Creating the encoder\n",
    "\n",
    "encoder_input = Input(shape=(None,), name=\"encoder_input\")\n",
    "embedding_size = 128\n",
    "encoder_embedding = Embedding(input_dim=num_words,\n",
    "                             output_dim= embedding_size,\n",
    "                             name=\"encoder_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "d21ce5322394d34dbfa05b7dcbd08d9c768f599f"
   },
   "outputs": [],
   "source": [
    "state_size = 512\n",
    "\n",
    "encoder_gru1 = GRU(state_size, name = \"encoder_gru1\", return_sequences=True)\n",
    "encoder_gru2 = GRU(state_size, name = \"encoder_gru2\", return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name = \"encoder_gru3\", return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "ade13ef714d7c3c5c3d9ce10c13487b8a25e2e32"
   },
   "outputs": [],
   "source": [
    "def connect_encoder():\n",
    "    net = encoder_input\n",
    "    \n",
    "    net = encoder_embedding(net)\n",
    "    \n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "    \n",
    "    encoder_output = net\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "73269ce0cfa8845dbaa55f3a87a23e15d76b8cee"
   },
   "outputs": [],
   "source": [
    "encoder_output = connect_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "5410b8028c24efe2a7b2a1bf99094399836f191e"
   },
   "outputs": [],
   "source": [
    "#Decoder\n",
    "decoder_initial_state = Input(shape=(state_size,),\n",
    "                             name=\"decoder_initial_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "913d1e7d5c501db0b4d7cac51c7b85be5bbf3b71"
   },
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None,), name = \"decoder_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "b66be204118a22345249494076cc586503c2275a"
   },
   "outputs": [],
   "source": [
    "decoder_embedding = Embedding(input_dim=num_words,\n",
    "                             output_dim=embedding_size,\n",
    "                             name=\"decoder_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "d1c6f282330f51571aa09333f41554a3b48304b2"
   },
   "outputs": [],
   "source": [
    "decoder_gru1 = GRU(state_size,name=\"decoder_gru1\",return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size,name=\"decoder_gru2\",return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size,name=\"decoder_gru3\",return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "26ffed4a06e7107d043bd99bb037f02688ec5810"
   },
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words,\n",
    "                     activation=\"linear\",\n",
    "                     name=\"decoder_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "1cc8dcc0dd7da1e9164afd107a348f54c6cf5004"
   },
   "outputs": [],
   "source": [
    "def connect_decoder(initial_state):\n",
    "    net = decoder_input\n",
    "    \n",
    "    net = decoder_embedding(net)\n",
    "    \n",
    "    net = decoder_gru1(net,initial_state=initial_state)\n",
    "    net = decoder_gru2(net,initial_state=initial_state)\n",
    "    net = decoder_gru3(net,initial_state=initial_state)\n",
    "    \n",
    "    decoder_output = decoder_dense(net)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "c44f0981ee9509ddf7b5ae50396280f3d53e4aad"
   },
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "model_train = Model(inputs=[encoder_input, decoder_input],\n",
    "                    outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "4a2a47c180cbae6926e0fd8e4adc307f6160be04"
   },
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                     outputs=[encoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "24c8d6851ee99def54af8cfdf1b7d80147b42a53"
   },
   "outputs": [],
   "source": [
    "decoder_output= connect_decoder(initial_state=decoder_initial_state)\n",
    "model_decoder = Model(inputs=[decoder_input,decoder_initial_state],\n",
    "                     outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "68bfa81364ff64d4085906d87f9b0442faeb680e"
   },
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                          logits=y_pred)\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    \n",
    "    #loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    #    logits=logits, labels=labels))\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "f590bf681f5ddb930112f05613a9e5def8ba1ccf"
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "61f4554be13b9fb32cb72d3cfabac40bc7102142"
   },
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype='int32', shape=(None,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "6e9bdd091b01cd959f012cbb49ec0449860eba66"
   },
   "outputs": [],
   "source": [
    "model_train.compile(optimizer=optimizer,\n",
    "                    loss=sparse_cross_entropy,\n",
    "                    target_tensors=[decoder_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "6d074b40a4d00f877e4e1d80fcaa16c80f49bb68"
   },
   "outputs": [],
   "source": [
    "path_checkpoint = '21_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "b605f565d9858e8071e5fa9d53c441ff9d4cc2a4"
   },
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "59abea9538e7ce6ca8a8511b26f2865bc7dcc7e1"
   },
   "outputs": [],
   "source": [
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='./21_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "dde00311480bf79edee7888e8a0e012bda35f0b1"
   },
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "b7465cdaab4f014b63d86dfcffa6bf9290f162a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to load checkpoint.\n",
      "Unable to open file (unable to open file: name = '21_checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "dae4a93b77f110cb93f30f4e9bbcd5dcef405ae8"
   },
   "outputs": [],
   "source": [
    "x_data = \\\n",
    "{\n",
    "    'encoder_input': encoder_input_data,\n",
    "    'decoder_input': decoder_input_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "d41e85fe8250cece0210390b853e75c6ace4a521"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9323, 9)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "2f214a0ce5253b225bd424d3c00ce1c6515e5b15"
   },
   "outputs": [],
   "source": [
    "y_data = \\\n",
    "{\n",
    "    'decoder_output': decoder_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "ce9da5575f9db5b5187b933a38c699a052709d93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0050792360828931325"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_split = 0.0050792360828931325\n",
    "validation_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "cfaa3b452fb29cc7d7c1dd2688e8e95da6a9e259"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9275 samples, validate on 48 samples\n",
      "Epoch 1/10\n",
      "9275/9275 [==============================] - 7s 737us/step - loss: 3.9435 - val_loss: 3.2558\n",
      "Epoch 2/10\n",
      "9275/9275 [==============================] - 1s 153us/step - loss: 2.8132 - val_loss: 3.2522\n",
      "Epoch 3/10\n",
      "9275/9275 [==============================] - 1s 151us/step - loss: 2.7333 - val_loss: 3.1096\n",
      "Epoch 4/10\n",
      "9275/9275 [==============================] - 1s 152us/step - loss: 2.6885 - val_loss: 3.3584\n",
      "Epoch 5/10\n",
      "9275/9275 [==============================] - 1s 151us/step - loss: 2.5237 - val_loss: 2.7169\n",
      "Epoch 6/10\n",
      "9275/9275 [==============================] - 1s 152us/step - loss: 2.2532 - val_loss: 2.6737\n",
      "Epoch 7/10\n",
      "9275/9275 [==============================] - 1s 151us/step - loss: 2.1557 - val_loss: 2.6535\n",
      "Epoch 8/10\n",
      "9275/9275 [==============================] - 1s 151us/step - loss: 2.0905 - val_loss: 2.6766\n",
      "Epoch 9/10\n",
      "9275/9275 [==============================] - 1s 152us/step - loss: 2.0352 - val_loss: 2.6370\n",
      "Epoch 10/10\n",
      "9275/9275 [==============================] - 1s 151us/step - loss: 1.9751 - val_loss: 2.6253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f136b923e10>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit(x=x_data,\n",
    "                y=y_data,\n",
    "                batch_size=512,\n",
    "                epochs=10,\n",
    "                validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "ff2e762a32609c443f4aacd7f1916a5d45387efd"
   },
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text=None):\n",
    "\n",
    "    input_tokens = sanskrit_tokenizer.text_to_tokens(text=input_text,\n",
    "                                                reverse=True,\n",
    "                                                padding=True)\n",
    "    \n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "\n",
    "    max_tokens = english_tokenizer.max_tokens\n",
    "    \n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "\n",
    "    token_int = token_start\n",
    "\n",
    "    output_text = ''\n",
    "\n",
    "    count_tokens = 0\n",
    "\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        x_data = \\\n",
    "        {\n",
    "            'decoder_initial_state': initial_state,\n",
    "            'decoder_input': decoder_input_data\n",
    "        }\n",
    "\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        \n",
    "        token_int = np.argmax(token_onehot)\n",
    "\n",
    "        sampled_word = english_tokenizer.token_to_word(token_int)\n",
    "\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        count_tokens += 1\n",
    "\n",
    "    output_tokens = decoder_input_data[0]\n",
    "    \n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    "    print(\"Translated text:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "d09bc8eca8e112e42f65fc10bdd42bbfc5ac7f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "apraapya \n",
      "\n",
      "Translated text:\n",
      " maternal eeee\n",
      "\n",
      "True output text:\n",
      "ssss  failing to attain eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 500\n",
    "translate(input_text=sanskrit_data[idx],\n",
    "          true_output_text=english_data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "33fdce726ca66f0d298eb838abc1cddc6588e1d0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "9e4d3949eee8c458024c4eb2bbe7b2fe1d58ed2e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "4602dffda3a2a37eb91e559f5361b284b99be9ee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "fb2c87fb41b1c9b50e7e94da253dbe6f1c5a5665"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "105ab55276ffb26f717fc23d593b06d8ee6ca8d9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "9d33e618ed13e40516af2976ded73aed41f2e6a4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "e09b2338ebe21f216afa5c0633e62b740fabbaff"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "c4a972a928930bcfe88a7db2db5f492ac5aca3ac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "46b6c2fb465015b0282af22f161dc6a08c1a936c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "e5759f931fc8f04b5809207a3225aa33f668f93f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
